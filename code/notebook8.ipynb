{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***EABL STOCK PERFORMANCE ANALYSIS WITH TIME SERIES AND SENTIMENT DATA***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***1.BUSINESS UNDERSTANDING***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROBLEM STATEMENT:**\n",
    "\n",
    "•\tEABL, a leading East African brewery company, has been experiencing stock price volatility and stagnation over the past 10 years, impacting shareholder confidence and investment stability.\n",
    "\n",
    "•\tRecent regulatory actions, such as the Kenya Revenue Authority's imposition of a policy requiring EABL to file returns within 24 hours, further exacerbate challenges faced by the company.\n",
    "\n",
    "•\tThe primary challenge is to devise data-driven strategies to stabilize EABL's stock performance, mitigate the impact of regulatory policies, and foster exponential growth in shareholder value.\n",
    "\n",
    "**BUSINESS OBJECTIVES:**\n",
    "\n",
    "•\tMaintain investor confidence and prevent shareholder withdrawal amidst stock price volatility and regulatory pressures.\n",
    "\n",
    "•\tStabilize EABL's stock performance and foster sustainable growth in stock value over time.\n",
    "\n",
    "•\tEnhance EABL's resilience to regulatory changes and policy disruptions while optimizing financial outcomes.\n",
    "\n",
    "**CONTEXT:**\n",
    "\n",
    "•\tEABL operates in the highly competitive and dynamic East African brewery industry, facing challenges such as changing consumer preferences, regulatory scrutiny, and economic fluctuations.\n",
    "\n",
    "•\tThe company's success is closely tied to its brand reputation, product quality, market positioning, and ability to adapt to evolving market conditions.\n",
    "\n",
    "•\tRegulatory interventions, such as the Kenya Revenue Authority's policy, can significantly impact EABL's operational efficiency, financial reporting processes, and investor perception.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***PROJECT PLAN***\n",
    "\n",
    "To address the challenge faced by EABL, a data-driven solution leveraging product sentiment analysis and time series data can be formulated\n",
    "\n",
    "## ***Data Collection and Preprocessing:***\n",
    "\n",
    "•\tGather historical stock market data for EABL, including daily closing prices, trading volume, and any relevant financial indicators.\n",
    "\n",
    "•\tAcquire sentiment data related to EABL's products from social media, customer reviews, and other relevant sources. Utilize natural language processing (NLP) techniques to extract sentiment scores.\n",
    "\n",
    "•\tOrganize and preprocess the collected data to ensure consistency and accuracy for analysis.\n",
    "\n",
    "## ***Time Series Analysis:***\n",
    "\n",
    "•\tConduct time series analysis on EABL's stock performance to identify patterns, trends, and factors influencing stock fluctuations over the past 18 years.\n",
    "\n",
    "•\tUse statistical methods such as trend analysis, and seasonal decomposition to understand the underlying dynamics of the stock market behavior.\n",
    "\n",
    "## ***Sentiment Analysis:***\n",
    "\n",
    "•\tPerform sentiment analysis on product-related data to gauge customer perception, satisfaction, and sentiment towards EABL's offerings.\n",
    "\n",
    "•\tClassify sentiments into positive, negative, or neutral categories and quantify the intensity of sentiment shifts over time.\n",
    "\n",
    "## ***Correlation Analysis:***\n",
    "\n",
    "•\tExplore the relationship between product sentiment scores and stock market performance using statistical techniques.\n",
    "\n",
    "•\tIdentify correlations and causal relationships between changes in sentiment and fluctuations in stock prices.\n",
    "\n",
    "## ***Predictive Modeling:***\n",
    "\n",
    "•\tDevelop predictive models using machine learning algorithms such as time series forecasting, or sentiment-based models.\n",
    "\n",
    "•\tPredict future stock price movements based on historical trends, sentiment analysis results, and other relevant factors.\n",
    "\n",
    "## ***Recommendations and Strategies:***\n",
    "\n",
    "•\tGenerate actionable insights and recommendations for EABL's stakeholders based on data-driven analysis.\n",
    "\n",
    "•\tPropose strategies to mitigate the negative effects of stock tanking and regulatory challenges.\n",
    "\n",
    "•\tSuggest proactive measures to enhance product quality, customer satisfaction, and brand reputation to stabilize and grow the company's stock value.\n",
    "\n",
    "## ***Continuous Monitoring and Adaptation:***\n",
    "\n",
    "•\tEstablish a framework for ongoing monitoring of stock market trends, sentiment dynamics, and policy developments.\n",
    "\n",
    "•\tContinuously refine and adapt the data-driven solution based on real-time data updates and feedback from stakeholders.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------\n",
    "# ***2.DATA UNDERSTANDING AND  ANALYSIS***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***2.1.TIME SERIES ANALYSIS***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Volatility Analysis:***\n",
    "\n",
    "Objective: Quantify EABL's stock volatility to assess investment risk. Develop a risk model to identify and quantify potential risks, providing investors with a comprehensive risk assessment tool.\n",
    "\n",
    "***Analysis of Abnormal Trade Volume:***\n",
    "\n",
    "Objective: Investigate anomalies in trade volumes, such as spikes or drops, to understand their causes and implications for market dynamics.\n",
    "\n",
    "***Dividends Analysis:***\n",
    "\n",
    "Objective: Evaluate EABL's dividend payout trends to inform investment decisions.\n",
    "\n",
    "***Trend and Seasonality Analysis:***\n",
    "\n",
    "Objective: Identify and analyze trends, seasonal patterns, and autocorrelations in EABL's stock data to forecast future movements.\n",
    "\n",
    "***Lag Analysis with Market Indicators:***\n",
    "\n",
    "Objective: Examine the impact of external market indicators (e.g., inflation rates, GDP, USD exchange rates, unemployment rates) on EABL's stock performance.\n",
    "\n",
    "***Stock Valuation:***\n",
    "\n",
    "Objective: Determine the intrinsic value of EABL stocks to assess their investment potential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'learn-env (Python 3.8.5)' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Let's start by loading the uploaded CSV file to understand its structure and perform some initial data cleaning.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "file_path =  r\"C:\\Users\\ADMIN\\OneDrive\\Desktop\\DEREK\\dsc-capstone-project_finale\\data\\processed\\merge_sentiments_final.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the dataset to understand its structure\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'learn-env (Python 3.8.5)' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Columns to drop\n",
    "columns_to_drop = ['text', 'No_of_likes', 'No_of_tweets', 'Unnamed: 0']\n",
    "\n",
    "# Dropping columns\n",
    "data = data.drop(columns=columns_to_drop)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains 4,955 entries with 18 columns, detailing EABL's stock performance and related economic indicatorse and broader market factors over a 18 year period, ending most recently on January 31, 2024. Here's a summary of the columns present in the dataset:\n",
    "\n",
    "- ***Date:** The date of the record, indicating when the stock information was recorded.\n",
    "\n",
    "- ***Open, High, Low, Close:*** Stock prices at the open, highest price, lowest price, and close of the trading day, respectively.\n",
    "\n",
    "- ***Average:*** The average price of the stock for the day.\n",
    "\n",
    "- ***Volume:*** The number of shares traded during the day.\n",
    "\n",
    "- ***Month, Year, Day:*** Broken down components of the date for easy access to month, year, and day.\n",
    "\n",
    "- ***Annual Average Inflation:*** Possibly an indicator meant to show inflation rates, though it seems to have missing values for the recent entries.\n",
    "\n",
    "- ***12-Month Inflation:*** Inflation rate over the past 12 months.\n",
    "\n",
    "- ***Mean:*** Exchange rates economic indicator.\n",
    "\n",
    "- ***Amount:*** Could be related to dividend amounts, though clarification is needed.\n",
    "\n",
    "- ***Dividends per share:*** The amount of dividend paid per share.\n",
    "\n",
    "- ***Earnings Per Share:*** Profit per share over a specified period, though it appears to have missing values in the latest entries.\n",
    "\n",
    "- ***Unemployment:*** The unemployment rate.\n",
    "\n",
    "- ***Interest Rates:*** The CBK interest rates\n",
    "\n",
    "Before proceeding with the analysis, we should perform some standard data cleaning steps, including:\n",
    "\n",
    "- ***Dropping the Annual Average Inflation***, as it appears to be an unnecessary column.\n",
    "\n",
    "- ***Parsing the Date column into a datetime format for time series analysis.***\n",
    "\n",
    "- Checking for and handling any missing values, especially in key columns like Earnings Per Share.\n",
    "\n",
    "- Ensuring numerical columns are of the correct data type for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'learn-env (Python 3.8.5)' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "# Drop the 'Annual Average Inflation' column\n",
    "\n",
    "data_cleaned = data.drop(columns=['Annual Average Inflation'])\n",
    "\n",
    "# Convert 'Date' column to datetime format\n",
    "data_cleaned['Date'] = pd.to_datetime(data_cleaned['Date'])\n",
    "\n",
    "# Check data types and look for missing values\n",
    "data_types = data_cleaned.dtypes\n",
    "missing_values = data_cleaned.isnull().sum()\n",
    "\n",
    "# Display data types and missing values information\n",
    "data_types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'learn-env (Python 3.8.5)' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have identified missing values in the following columns:\n",
    "\n",
    "- Dividends Per Share with 40 missing values\n",
    "\n",
    "- Earnings Per Share each have 41 missing values.\n",
    "\n",
    "- Mean has 4 missing values.\n",
    "\n",
    "For Earnings Per Share, missing values will be imputed due to  their significance to the analysis. \n",
    "\n",
    "The Dividends Per Share  will be filled using backward fill \n",
    "\n",
    "The \"mean\" column will be filled with interpolated values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'learn-env (Python 3.8.5)' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Impute missing values for 'Earnings Per Share' with the column's mean\n",
    "eps_mean = data_cleaned['Earnings Per Share'].mean()\n",
    "data_cleaned['Earnings Per Share'].fillna(eps_mean, inplace=True)\n",
    "\n",
    "# Use linear interpolation to fill missing values for'Mean' and backward fill forDividends Per Share\n",
    "data_cleaned['Dividends per share'] = data_cleaned['Dividends per share'].fillna(method='bfill')\n",
    "data_cleaned['Mean'].interpolate(method='linear', inplace=True)\n",
    "\n",
    "# Verify that missing values have been addressed\n",
    "missing_values_after_imputation = data_cleaned.isnull().sum()\n",
    "\n",
    "missing_values_after_imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Volatility Analysis***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volatility is a statistical measure of the dispersion of returns for a given security or market index. In most cases, the higher the volatility, the riskier the security. We'll calculate the historical volatility of EABL's stock, using the standard deviation of daily returns as a proxy for volatility.\n",
    "\n",
    "For this purpose, we need to:\n",
    "\n",
    "- Calculate the daily returns of EABL's stock.\n",
    "- Define and implement a method to calculate the rolling standard deviation of these returns, which represents the volatility.\n",
    "- Visualize the volatility over time to identify any patterns or trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'learn-env (Python 3.8.5)' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class StockVolatility:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.returns = None\n",
    "    \n",
    "    def calculate_returns(self):\n",
    "        \"\"\"Calculate daily returns from the Close prices.\"\"\"\n",
    "        self.data['Returns'] = self.data['Close'].pct_change()\n",
    "        self.returns = self.data['Returns']\n",
    "    \n",
    "    def calculate_volatility(self):\n",
    "        \"\"\"Calculate the annualized volatility of the stock.\"\"\"\n",
    "        daily_volatility = self.returns.std()\n",
    "        annualized_volatility = daily_volatility * np.sqrt(252)  # Assuming 252 trading days in a year\n",
    "        return annualized_volatility\n",
    "    \n",
    "    def plot_volatility(self):\n",
    "        \"\"\"Plot the rolling 30-day volatility of the stock.\"\"\"\n",
    "        rolling_volatility = self.returns.rolling(window=30).std() * np.sqrt(252)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        rolling_volatility.plot()\n",
    "        plt.title('30-Day Rolling Volatility of EABL Stock')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Annualized Volatility')\n",
    "        plt.show()\n",
    "\n",
    "# Instantiate the StockVolatility class and perform the analysis\n",
    "eabl_volatility = StockVolatility(data_cleaned)\n",
    "eabl_volatility.calculate_returns()\n",
    "\n",
    "# Calculate and display the annualized volatility\n",
    "annualized_volatility = eabl_volatility.calculate_volatility()\n",
    "print(f\"Annualized Volatility: {annualized_volatility:.2%}\")\n",
    "\n",
    "# Plot the 30-day rolling volatility\n",
    "eabl_volatility.plot_volatility()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Observation:***\n",
    "\n",
    "The average volatility rate for EABL's stock over the period analyzed is approximately 32.65%. This rate indicates the average level of fluctuation in the stock's daily returns on an annualized basis, reflecting the stock's risk profile. A volatility rate of this magnitude suggests that EABL's stock has experienced moderate to high levels of price fluctuations, which is a crucial factor for investors to consider in their risk assessment and investment strategy. ​​\n",
    "\n",
    "## ***Insights:***\n",
    "\n",
    "- ***For Risk-Averse Investors:*** Given the relatively high volatility of EABL's stock, more cautious investors might prefer to look for lower-volatility investment opportunities unless they are comfortable with the associated risk levels.\n",
    "\n",
    "- ***For Risk-Tolerant Investors:*** Those willing to accept higher volatility in exchange for the potential of significant gains might find EABL's stock to be an attractive option. However, it is crucial to implement robust risk management strategies, such as diversification and setting stop-loss orders.\n",
    "\n",
    "- ***Market Timing:*** Investors might also use volatility trends to time their market entry and exit. Periods of increasing volatility could signal opportunities for high returns, but they should be approached with caution and a clear understanding of potential downside risks.\n",
    "\n",
    "\n",
    "The analysis of EABL's stock volatility reveals a substantial level of risk, characterized by the potential for significant price fluctuations. Investors should consider their risk tolerance and investment horizon when deciding to invest in EABL stocks. By understanding and planning for volatility, investors can better position themselves to navigate the uncertainties of the stock market and capitalize on potential opportunities. ​"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Abnormal Trade Volume Analysis:***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We'll focus on identifying and understanding instances where EABL's trading volume significantly deviates from its typical levels. Such spikes can indicate a variety of market conditions or events, such as new product launches, earnings reports, changes in management, or broader economic news impacting the stock. \n",
    "\n",
    "- Detecting these spikes can provide valuable insights into potential market sentiment changes or upcoming volatility\n",
    "\n",
    "- We'll use a z-score approach to identify outliers in trading volumes. The z-score measures the number of standard deviations an element is from the mean. Typically, a z-score above 2 or below -2 is considered abnormal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'learn-env (Python 3.8.5)' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "class TradeVolumeAnalysis:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def detect_spikes(self, threshold=2):\n",
    "        \"\"\"Identify days with abnormal trade volume spikes.\n",
    "        \n",
    "        Args:\n",
    "            threshold (float): The number of standard deviations from the mean to consider a spike.\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame: A subset of the original DataFrame with only the days of abnormal volume spikes.\n",
    "        \"\"\"\n",
    "        self.data['Volume_Mean'] = self.data['Volume'].rolling(window=30).mean()\n",
    "        self.data['Volume_Std'] = self.data['Volume'].rolling(window=30).std()\n",
    "        self.data['Z_Score'] = (self.data['Volume'] - self.data['Volume_Mean']) / self.data['Volume_Std']\n",
    "        \n",
    "        # Filter rows where the Z-score is above the threshold\n",
    "        spikes = self.data[self.data['Z_Score'] > threshold]\n",
    "        return spikes\n",
    "    \n",
    "    def plot_volume_spikes(self):\n",
    "        \"\"\"Plot the daily trade volume with highlights on days of abnormal spikes.\"\"\"\n",
    "        plt.figure(figsize=(20, 8))\n",
    "        plt.plot(self.data['Date'], self.data['Volume'], label='Daily Volume')\n",
    "        \n",
    "        # Detect spikes\n",
    "        spikes = self.detect_spikes()\n",
    "        \n",
    "        plt.scatter(spikes['Date'], spikes['Volume'], color='red', label='Volume Spikes')\n",
    "        plt.title('EABL Trade Volume with Abnormal Spikes Highlighted')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Trade Volume')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        return spikes\n",
    "\n",
    "# Instantiate the TradeVolumeAnalysis class and perform the analysis\n",
    "volume_analysis = TradeVolumeAnalysis(data_cleaned)\n",
    "spikes_df = volume_analysis.plot_volume_spikes()\n",
    "\n",
    "# Display the dates and volumes of detected spikes for review\n",
    "spikes_df[['Date', 'Volume', 'Z_Score']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By calculating the z-score for each day's trading volume and flagging those with scores exceeding a threshold of 2, we pinpointed days with significantly higher than average trading activity.\n",
    "\n",
    "## ***Examples of Detected Abnormal Volumes:***\n",
    "- December 21, 2023: Volume of 305,500 with a z-score of 5.29.\n",
    "- October 11, 2023: Volume of 283,900 with a z-score of 5.27\n",
    "- November 16, 2006: Volume of 480,300 with a z-score of 2.56.\n",
    "- October 31 2006: Volume of 433,800 with a z-score of 2.37\n",
    "\n",
    "## ***Observations***\n",
    "- ***Significance of Spikes:*** The days highlighted with red in the plot indicate significant deviations in trading volume, suggesting notable events or news impacting investor sentiment and actions.\n",
    "\n",
    "- ***Potential Causes:*** These spikes could be attributed to various factors such as earnings announcements, strategic corporate decisions, regulatory news, or market rumors. Further investigation into the specific dates could provide context to these volume spikes, offering deeper insights into their causes and implications.\n",
    "\n",
    "- ***Investment Strategy Considerations:*** Abnormal volume spikes can serve as a precursor to price movements. An increase in volume often precedes price volatility, suggesting that the stock could be on the verge of a breakout or breakdown.\n",
    "\n",
    "## ***Rationale:***\n",
    "Trade volume spikes can result from a variety of causes, including but not limited to:\n",
    "\n",
    "- ***Corporate news or events:*** Announcements related to earnings, mergers, acquisitions, or other significant corporate events.\n",
    "\n",
    "- ***Market sentiment:*** Changes in investor sentiment due to broader market or economic news.\n",
    "\n",
    "- ***Institutional trading activity:*** Large trades executed by institutional investors or hedge funds.\n",
    "\n",
    "## ***Insights:***\n",
    "\n",
    "- ***Market Surveillance:** Investors and analysts should closely monitor news and events around the dates of these volume spikes to understand the underlying causes and potential market reactions.\n",
    "\n",
    "- **Investment Strategy:*** Abnormal volume days can sometimes precede significant price movements. Investors might use this information to adjust their investment strategies, potentially taking positions before expected moves or using the information to time exits.\n",
    "\n",
    "- ***Risk Assessment:*** Sudden increases in trade volume, especially when not accompanied by significant news, might indicate speculative trading or manipulation. Investors should be cautious and conduct thorough research before making investment decisions based on volume spikes alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Dividends Analysis***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Dividends Analysis, we'll focus on understanding the trends and patterns in East African Breweries Limited (EABL)'s dividend payouts. Dividends are a critical component for investors, especially those seeking income from their investments. Analyzing the dividend history can provide insights into the company's profitability, stability, and management's confidence in future earnings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'learn-env (Python 3.8.5)' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "class DividendsAnalysis:\n",
    "    \"\"\"\n",
    "    A class for analyzing dividend payout trends.\n",
    "    \"\"\"\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        # Ensure there's a 'Year' column for annual analysis\n",
    "        if 'Year' not in self.data.columns:\n",
    "            self.data['Date'] = pd.to_datetime(self.data['Date'])\n",
    "            self.data['Year'] = self.data['Date'].dt.year\n",
    "        self.dividend_data = self.data[['Year', 'Dividends per share']].drop_duplicates().dropna()\n",
    "        \n",
    "    def calculate_dividend_growth(self):\n",
    "        \"\"\"\n",
    "        Calculate the year-over-year growth rate of dividends per share.\n",
    "        \"\"\"\n",
    "        self.dividend_data['Growth'] = self.dividend_data['Dividends per share'].pct_change() * 100\n",
    "        return self.dividend_data\n",
    "    \n",
    "    def plot_dividend_trends(self):\n",
    "        \"\"\"\n",
    "        Visualize the trends in dividend payouts over time, including growth rates.\n",
    "        \"\"\"\n",
    "        fig, ax1 = plt.subplots(figsize=(14, 7))\n",
    "\n",
    "        color = 'tab:red'\n",
    "        ax1.set_xlabel('Year')\n",
    "        ax1.set_ylabel('Dividends per Share', color=color)\n",
    "        ax1.plot(self.dividend_data['Year'], self.dividend_data['Dividends per share'], color=color, label='Dividends per Share')\n",
    "        ax1.tick_params(axis='y', labelcolor=color)\n",
    "        \n",
    "        ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "        color = 'tab:blue'\n",
    "        ax2.set_ylabel('Growth Rate (%)', color=color)  # we already handled the x-label with ax1\n",
    "        ax2.plot(self.dividend_data['Year'], self.dividend_data['Growth'], color=color, label='Growth Rate')\n",
    "        ax2.tick_params(axis='y', labelcolor=color)\n",
    "        \n",
    "        fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "        plt.title('EABL Dividends per Share and Growth Rate Over Time')\n",
    "        plt.show()\n",
    "\n",
    "# Initialize the class with the cleaned data (assuming data_cleaned is the dataset to use)\n",
    "dividends_analysis = DividendsAnalysis(data_cleaned)\n",
    "\n",
    "# Calculate dividend growth rates\n",
    "dividend_growth_data = dividends_analysis.calculate_dividend_growth()\n",
    "\n",
    "# Plotting dividend trends and growth rates\n",
    "dividends_analysis.plot_dividend_trends()\n",
    "\n",
    "# Displaying the calculated growth rates for review\n",
    "dividend_growth_data[['Year', 'Dividends per share', 'Growth']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Observation:***\n",
    "The analysis reveals fluctuating trends in EABL's dividends per share over the recent years, with significant growth seen in 2022 followed by a drop in 2023, and stabilization in 2024. Notably, there's an apparent discrepancy in 2024 with two entries, one indicating a continuation of the previous dividend rate (5.5) and another showing a drop to 0.0, which suggests a discontinuation or a special case affecting dividend payout.\n",
    "\n",
    "## ***Rationale:***\n",
    "\n",
    "- ***Growth in 2022:*** The doubling of dividends in 2022 could indicate strong financial performance or a strategic decision to return more capital to shareholders.\n",
    "\n",
    "- ***Drop in 2023:*** The subsequent drop might reflect adjustments in financial strategy, a reallocation of profits towards growth investments, or a response to external economic pressures.\n",
    "\n",
    "- ***Situation in 2024:*** The mixed signals in 2024 require further investigation. The record showing a dividend of 0 could be due to an error, a temporary suspension of dividends, or a placeholder until final decisions are made.\n",
    "\n",
    "## ***Insights:***\n",
    "\n",
    "- ***Investment Consideration:*** Investors should closely monitor EABL's financial reports and shareholder communications for insights into dividend policies and sustainability. The historical volatility in dividends suggests a need for caution, especially for those relying on dividend income.\n",
    "\n",
    "- ***Long-Term Strategy:*** Given the fluctuations, a long-term perspective is advisable when incorporating EABL stocks into a portfolio. Consider the overall yield and the potential for capital appreciation.\n",
    "\n",
    "- ***Further Research:*** The anomaly in 2024 warrants further investigation. Investors should seek additional information regarding the company's dividend policy and future outlook to understand the potential impact on investment returns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Trend Analysis***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Trend Analysis of East African Breweries Limited (EABL)'s stock, we will examine the historical price data to identify any prevailing trends, seasonal patterns, and any potential autocorrelation in the stock prices over time. Understanding these aspects can help in forecasting future price movements and in making informed investment decisions.\n",
    "\n",
    "To conduct this analysis, we will use the TrendAnalysis class with methods for:\n",
    "\n",
    "- ***Trend Detection:*** Identify long-term movements in the stock prices.\n",
    "- ***Seasonality Analysis:*** Examine the data for regular, repeating patterns that occur over a known period.\n",
    "- ***Autocorrelation Analysis:*** Determine if the stock prices are correlated with their past values, which can be useful for predictive modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'learn-env (Python 3.8.5)' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries for trend analysis\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "\n",
    "class TrendAnalysis:\n",
    "    \"\"\"\n",
    "    A class to perform trend analysis on stock prices.\n",
    "    \"\"\"\n",
    "    def __init__(self, data):\n",
    "        self.data = data.copy()\n",
    "        self.data['Date'] = pd.to_datetime(self.data['Date'])\n",
    "        self.data.set_index('Date', inplace=True)\n",
    "        \n",
    "    def decompose_series(self, model='additive'):\n",
    "        \"\"\"\n",
    "        Decompose the time series into its trend, seasonal, and residual components.\n",
    "        \"\"\"\n",
    "        # Assuming 'Close' price for analysis and a frequency of 252 trading days per year\n",
    "        decomposition = seasonal_decompose(self.data['Close'], model=model, period=252)\n",
    "        return decomposition\n",
    "    \n",
    "    def plot_decomposition(self, decomposition):\n",
    "        \"\"\"\n",
    "        Plot the decomposed components of the time series.\n",
    "        \"\"\"\n",
    "        fig, (ax1, ax2, ax3, ax4) = plt.subplots(4, 1, figsize=(14, 10))\n",
    "        decomposition.observed.plot(ax=ax1)\n",
    "        ax1.set_title('Observed')\n",
    "        decomposition.trend.plot(ax=ax2)\n",
    "        ax2.set_title('Trend')\n",
    "        decomposition.seasonal.plot(ax=ax3)\n",
    "        ax3.set_title('Seasonal')\n",
    "        decomposition.resid.plot(ax=ax4)\n",
    "        ax4.set_title('Residual')\n",
    "        plt.tight_layout()\n",
    "    \n",
    "    def plot_autocorrelation(self):\n",
    "        \"\"\"\n",
    "        Plot the autocorrelation of the 'Close' price to identify any autocorrelation patterns.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(14, 7))\n",
    "        autocorrelation_plot(self.data['Close'])\n",
    "        plt.title('Autocorrelation of EABL Closing Prices')\n",
    "        plt.show()\n",
    "\n",
    "# Initialize the class with the cleaned data\n",
    "trend_analysis = TrendAnalysis(data_cleaned)\n",
    "\n",
    "# Decompose the series\n",
    "decomposition = trend_analysis.decompose_series()\n",
    "\n",
    "# Plotting the decomposition\n",
    "trend_analysis.plot_decomposition(decomposition)\n",
    "\n",
    "# Plotting the autocorrelation\n",
    "trend_analysis.plot_autocorrelation()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Decomposition Analysis:***\n",
    "- ***Observed:*** The observed data shows fluctuations over the years with a noticeable decline in values after 2013.\n",
    "\n",
    "- ***Trend:*** There is a clear upward trend until around 2013, after which there is a decline. This suggests EABL's stock had been increasing over time but has started to decrease recently.\n",
    "\n",
    "- ***Seasonal:*** The seasonal component reveals any repeating patterns or cycles in the stock prices over a specified period. In this case, the analysis aimed to uncover any monthly or quarterly patterns, but the visualization suggests that seasonality may not be a dominant feature in EABL's stock price movements.\n",
    "\n",
    "- ***Residual:*** Residuals are relatively stable with some spikes indicating instances where the price significantly deviated from the trend and seasonal components.\n",
    "\n",
    "### ***Autocorrelation Analysis:***\n",
    "- The autocorrelation plot indicates how the stock's closing prices correlate with its past values. Peaks in the plot suggest that the stock prices have some degree of predictability based on their past values. In this case, the autocorrelation plot shows a gradual decline, suggesting that past prices have some influence on future prices, but this influence diminishes over time.\n",
    "\n",
    "### ***Observations:***\n",
    "- ***Trend Identification:*** Understanding the long-term trend is crucial for making investment decisions, especially for buy-and-hold investors. The trend analysis suggests a direction that the stock might continue to follow.\n",
    "\n",
    "- ***Seasonality Insights:*** While the analysis does not strongly indicate significant seasonality in EABL's stock prices, investors should still consider other cyclical factors that might affect the stock market in general.\n",
    "\n",
    "- ***Predictive Modeling:*** The autocorrelation analysis provides a basis for developing predictive models. Stocks with significant autocorrelation can often be forecasted more accurately using past price data.\n",
    "\n",
    "### ***Insights:***\n",
    "- ***Long-Term Investments:*** Investors looking for long-term opportunities should pay close attention to the trend component to align their investment strategies with the overall direction of the stock.\n",
    "\n",
    "- ***Trading Strategies:*** Traders might use autocorrelation insights to develop short-term trading strategies, capitalizing on the predictability of price movements.\n",
    "\n",
    "- ***Diversification:*** The absence of strong seasonality suggests that EABL's stock might not offer diversification benefits based on seasonal trading strategies alone. Investors might need to look for other stocks or assets with clearer seasonal patterns for diversification purposes.\n",
    "\n",
    "Investigate the causes of the decline post-2013 and develop strategies to mitigate or reverse this trend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Lag Analysis***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Lag Analysis, we'll examine how various market indicators, specifically inflation rates, GDP, USD exchange rates, and unemployment rates, impact East African Breweries Limited (EABL)'s stock prices. Understanding these relationships can provide insights into how external economic factors influence stock performance, which is essential for making informed investment decisions.\n",
    "\n",
    "To conduct this analysis, we will enhance the LagAnalysis class to include methods for:\n",
    "\n",
    "- ***Correlation Analysis:*** To identify the strength and direction of the relationship between EABL's stock prices and each market indicator.\n",
    "- ***Lag Analysis:*** To determine if there's a time lag between changes in market indicators and their impact on EABL's stock prices, indicating predictability.\n",
    "\n",
    "This analysis will involve statistical tests to measure correlation and lag effects, providing a clearer picture of how external economic factors influence EABL's stock performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'learn-env (Python 3.8.5)' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "class LagAnalysis:\n",
    "    \"\"\"\n",
    "    A class to perform lag analysis on EABL's stock prices against various market indicators.\n",
    "    \"\"\"\n",
    "    def __init__(self, stock_data, indicators_data):\n",
    "        self.stock_data = stock_data\n",
    "        self.indicators_data = indicators_data\n",
    "        \n",
    "    def calculate_correlation(self, indicator_column):\n",
    "        \"\"\"\n",
    "        Calculate the Pearson correlation coefficient between EABL's closing stock prices and a given market indicator.\n",
    "        \"\"\"\n",
    "        # Ensuring alignment in dates between stock data and indicators data\n",
    "        combined_data = self.stock_data[['Close']].merge(self.indicators_data[[indicator_column]], left_index=True, right_index=True, how='inner')\n",
    "        correlation, p_value = pearsonr(combined_data['Close'], combined_data[indicator_column])\n",
    "        return correlation, p_value\n",
    "    \n",
    "    def perform_lag_analysis(self, indicator_column, max_lag=12):\n",
    "        \"\"\"\n",
    "        Perform lag analysis to determine the time lag effect of changes in market indicators on EABL's stock prices.\n",
    "        \"\"\"\n",
    "        # Aligning the data based on dates for comparison\n",
    "        combined_data = self.stock_data[['Close']].merge(self.indicators_data[[indicator_column]], left_index=True, right_index=True, how='inner')\n",
    "        \n",
    "        lags = range(0, max_lag+1)\n",
    "        autocorrelations = [combined_data['Close'].autocorr(lag=lag) for lag in lags]\n",
    "        \n",
    "        return lags, autocorrelations\n",
    "\n",
    "# Preparing indicators data \n",
    "indicators_columns = ['12-Month Inflation', 'Unemployment', 'Mean', 'Interest rates']  \n",
    "indicators_data = data_cleaned.set_index('Date')[indicators_columns]\n",
    "\n",
    "# Initializing the LagAnalysis class\n",
    "lag_analysis = LagAnalysis(data_cleaned.set_index('Date'), indicators_data)\n",
    "\n",
    "# Iterate over each indicator\n",
    "for indicator in indicators_columns:\n",
    "    # Correlation analysis\n",
    "    correlation, p_value = lag_analysis.calculate_correlation(indicator)\n",
    "    print(f\"Correlation with {indicator}: {correlation}, P-value: {p_value}\")\n",
    "\n",
    "    # Performing lag analysis\n",
    "    lags, autocorrelations = lag_analysis.perform_lag_analysis(indicator)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.stem(lags, autocorrelations)\n",
    "    plt.title(f'Lag Analysis of Market Indicators Impact on EABL Stock Prices ({indicator})')\n",
    "    plt.xlabel('Lag')\n",
    "    plt.ylabel('Autocorrelation')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Lag Analysis Results:***\n",
    "***12-Month Inflation:***\n",
    "\n",
    "- Correlation: -0.0966\n",
    "\n",
    "- P-value: 6.135e-21\n",
    "\n",
    "***Unemployment:***\n",
    "\n",
    "- Correlation: -0.5827\n",
    "\n",
    "- P-value: 0.0\n",
    "\n",
    "***Mean:***\n",
    "\n",
    "- Correlation: -0.5053\n",
    "\n",
    "- P-value: 0.0\n",
    "\n",
    "***Interest rates:***\n",
    "\n",
    "- Correlation: 0.0453\n",
    "\n",
    "- P-value: 1.118e-05\n",
    "\n",
    "## ***Observations:***\n",
    "\n",
    "- The correlation between EABL's stock prices and the 12-Month Inflation is weakly negative, suggesting a slight inverse relationship.\n",
    "\n",
    "- Conversely, there's a moderately strong negative correlation between EABL's stock prices and Unemployment and Mean, indicating a more significant inverse relationship.\n",
    "\n",
    "- The correlation with Interest rates is positive but very weak.\n",
    "\n",
    "## ***Interpretation:***\n",
    "\n",
    "- The significant negative correlations with Unemployment and Mean imply that as these indicators increase, EABL's stock prices tend to decrease, and vice versa. This suggests that factors affecting employment rates and the mean of certain economic metrics impact EABL's stock performance more prominently.\n",
    "\n",
    "- The weak negative correlation with 12-Month Inflation suggests a minor inverse relationship between inflation and EABL's stock prices. While statistically significant, the magnitude of this correlation is relatively small.\n",
    "\n",
    "- The positive correlation with Interest rates indicates a weak positive relationship, implying that higher interest rates might correspond to slightly higher EABL stock prices, though the effect is not very pronounced.\n",
    "\n",
    "## ***Insights:***\n",
    "\n",
    "- ***Inflation Sensitivity:*** While there's a significant negative correlation with Unemployment and Mean, indicating their importance in influencing EABL's stock prices, the correlation with inflation is weaker. Investors should still consider inflation trends but may find Unemployment and Mean to be more impactful factors.\n",
    "\n",
    "- ***Investment Strategy:*** Investors may want to pay close attention to unemployment rates and economic mean indicators as they appear to have a more significant impact on EABL's stock prices. Hedging strategies might focus more on these indicators rather than inflation alone.\n",
    "\n",
    "- ***Risk Management:*** Incorporating unemployment and economic mean indicators into risk management strategies could be crucial, as they show a stronger relationship with EABL's stock prices. Diversification strategies should consider these factors alongside inflation and interest rate considerations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------\n",
    "# ***3.PREDICTIVE MODELING***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***3.1.MODEL DEVELOPMENT***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "• Develop predictive models using machine learning algorithms such as  time series \n",
    "forecasting, \n",
    "\n",
    "• Predict future stock price movements based on historical trends, and \n",
    "other relevant factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'learn-env (Python 3.8.5)' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "data_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'learn-env (Python 3.8.5)' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Columns to drop\n",
    "columns_to_drop = [ 'Open' ,'High','Low', 'Average','Month','Year','Day','Amount',\t'Dividends per share',\t'Earnings Per Share','Returns', 'Volume_Mean','Volume_Std','Z_Score']\n",
    "\n",
    "# Dropping columns\n",
    "m1data = data_cleaned.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'learn-env (Python 3.8.5)' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "m1data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'learn-env (Python 3.8.5)' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Columns to drop\n",
    "columns_to_drop = [ 'Volume' ,'12-Month Inflation','Mean', 'Unemployment','Interest rates']\n",
    "\n",
    "# Dropping columns\n",
    "m2data = m1data.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'learn-env (Python 3.8.5)' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "class LSTMTimeSeriesModel:\n",
    "    def __init__(self, look_back=5, epochs=20, batch_size=20, lstm_units=50):\n",
    "        self.look_back = look_back\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.lstm_units = lstm_units\n",
    "        self.model = None\n",
    "        self.scaler = MinMaxScaler()\n",
    "\n",
    "    def preprocess_data(self, data, date_column='Date'):\n",
    "        data[date_column] = pd.to_datetime(data[date_column])\n",
    "        data.set_index(date_column, inplace=True)\n",
    "        \n",
    "        # Normalize the data\n",
    "        self.data_scaled = self.scaler.fit_transform(data)\n",
    "        \n",
    "        # Split the dataset into training and testing sets\n",
    "        self.split_idx = int(len(self.data_scaled) * 0.8)\n",
    "        self.train, self.test = self.data_scaled[:self.split_idx], self.data_scaled[self.split_idx:]\n",
    "    \n",
    "    def build_model(self, input_shape):\n",
    "        self.model = Sequential([\n",
    "            LSTM(self.lstm_units, activation='relu', input_shape=input_shape),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        self.model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "        \n",
    "    def train_model(self):\n",
    "        # Prepare data for LSTM model using TimeseriesGenerator\n",
    "        train_generator = TimeseriesGenerator(self.train, self.train, length=self.look_back, batch_size=self.batch_size)\n",
    "        input_shape = (self.look_back, self.train.shape[1])\n",
    "        \n",
    "        self.build_model(input_shape)\n",
    "        self.model.fit(train_generator, epochs=self.epochs, verbose=1)\n",
    "\n",
    "    def predict(self):\n",
    "        test_generator = TimeseriesGenerator(self.test, self.test, length=self.look_back, batch_size=1)\n",
    "        predictions = self.model.predict(test_generator)\n",
    "        \n",
    "        # Invert scaling for prediction data\n",
    "        predictions_inverted = self.scaler.inverse_transform(predictions)\n",
    "        \n",
    "        return predictions_inverted\n",
    "    \n",
    "    def visualize_and_evaluate(self,predictions, actual, start_idx=0):\n",
    "        \"\"\"\n",
    "        Visualizes the LSTM model predictions against the actual values and calculates performance metrics.\n",
    "\n",
    "        :param predictions: The predicted values returned by the model.\n",
    "        :param actual: The actual values to compare against the predictions.\n",
    "        :param start_idx: The starting index from which to display the actual values. This accounts for the look_back period.\n",
    "        \"\"\"\n",
    "        # Ensure the actual values start from the correct index to align with predictions\n",
    "        actual_aligned = actual[start_idx:]\n",
    "\n",
    "        # Calculate metrics\n",
    "        mae = mean_absolute_error(actual_aligned, predictions)\n",
    "        rmse = np.sqrt(mean_squared_error(actual_aligned, predictions))\n",
    "\n",
    "        # Plotting\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(actual_aligned, label='Actual', color='blue', marker='o')\n",
    "        plt.plot(predictions, label='Predicted', color='red', linestyle='--', marker='x')\n",
    "        plt.title('LSTM Time Series Prediction')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Value')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # Print metrics\n",
    "        print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "        print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "# Usage\n",
    "data = pd.DataFrame(m2data) \n",
    "\n",
    "lstm_model = LSTMTimeSeriesModel(look_back=5, epochs=20, batch_size=20, lstm_units=50)\n",
    "lstm_model.preprocess_data(data, date_column='Date')\n",
    "lstm_model.train_model()\n",
    "predictions = lstm_model.predict()\n",
    "\n",
    "actual = lstm_model.scaler.inverse_transform(lstm_model.test)[lstm_model.look_back:]\n",
    "\n",
    "#call visualize_and_evaluate on the lstm_model instance\n",
    "lstm_model.visualize_and_evaluate(predictions, actual, start_idx=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Observations:***\n",
    "\n",
    "- The graph shows a close alignment between the actual and predicted values, as indicated by the overlapping blue (actual) and red (predicted) points.\n",
    "\n",
    "- The Mean Absolute Error (MAE) is 3.0970, and the Root Mean Squared Error (RMSE) is 4.5802. These metrics suggest that on average, the model's predictions are about 3.0970 units off from the actual values, with some larger errors as indicated by the RMSE.\n",
    "\n",
    "- The loss during training decreases significantly from epoch 1 to epoch 2 and continues to decrease, albeit at a slower rate, through subsequent epochs. The final loss is quite low, suggesting the model's predictions are relatively close to the actual values in the training set.\n",
    "\n",
    "## ***Rationale for Using LSTM:***\n",
    "\n",
    "- LSTMs are particularly well-suited for time series forecasting due to their ability to capture long-term dependencies in sequential data. This characteristic is beneficial for stock price predictions, which are influenced by complex patterns and trends over time.\n",
    "\n",
    "- LSTMs can handle data with varying time intervals, missing values, and time-based patterns, making them robust for real-world time series prediction tasks.\n",
    "\n",
    "## ***Interpretation of Observations:***\n",
    "\n",
    "- The training process appears to be stable, with the loss decreasing consistently, indicating that the model is learning from the training data effectively.\n",
    "\n",
    "-  The model's performance metrics, MAE and RMSE, show that the model has a good predictive capability. However, the actual usefulness of these errors depends on the scale of the stock prices and the specific application or trading strategy where the model is used.\n",
    "\n",
    "- The model might benefit from further tuning, such as hyperparameter optimization, to potentially achieve even lower error rates.\n",
    "\n",
    "## ***Insights:***\n",
    "\n",
    "- The LSTM model is capturing the trend of the stock prices well, as indicated by the plot. This suggests that it could be a valuable tool in a broader stock price prediction or trading system.\n",
    "\n",
    "- The relatively low MAE and RMSE values imply that the model could be used as a reliable forecast in a decision-making process, assuming the errors are within an acceptable range for the application.\n",
    "\n",
    "- Given the nature of stock prices, which are influenced by many unpredictable factors, the performance of the LSTM is quite promising. However, it's crucial to validate the model on out-of-sample data and consider other forms of analysis, such as fundamental and event-driven analysis, for a comprehensive stock evaluation strategy.\n",
    "\n",
    "- It's important to note that while the model performs well on the given dataset, the real test of its utility will be in how it performs in live or more recent conditions where the model hasn't seen the data during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'learn-env (Python 3.8.5)' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import pickle\n",
    "\n",
    "class LSTMTimeSeriesModel:\n",
    "    def __init__(self, look_back=5, epochs=20, batch_size=20, lstm_units=50):\n",
    "        self.look_back = look_back\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.lstm_units = lstm_units\n",
    "        self.model = None\n",
    "        self.scaler = MinMaxScaler()\n",
    "\n",
    "    def preprocess_data(self, data, date_column='Date'):\n",
    "        data[date_column] = pd.to_datetime(data[date_column])\n",
    "        data.set_index(date_column, inplace=True)\n",
    "        \n",
    "        # Normalize the data\n",
    "        self.data_scaled = self.scaler.fit_transform(data)\n",
    "        \n",
    "        # Split the dataset into training and testing sets\n",
    "        self.split_idx = int(len(self.data_scaled) * 0.8)\n",
    "        self.train, self.test = self.data_scaled[:self.split_idx], self.data_scaled[self.split_idx:]\n",
    "    \n",
    "    def build_model(self, input_shape):\n",
    "        self.model = Sequential([\n",
    "            LSTM(self.lstm_units, activation='relu', input_shape=input_shape),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        self.model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "        \n",
    "    def train_model(self):\n",
    "        # Prepare data for LSTM model using TimeseriesGenerator\n",
    "        train_generator = TimeseriesGenerator(self.train, self.train[:, 0], length=self.look_back, batch_size=self.batch_size)\n",
    "        input_shape = (self.look_back, self.train.shape[1])\n",
    "        \n",
    "        self.build_model(input_shape)\n",
    "        self.model.fit(train_generator, epochs=self.epochs, verbose=1)\n",
    "\n",
    "    def predict(self):\n",
    "        test_generator = TimeseriesGenerator(self.test, self.test[:, 0], length=self.look_back, batch_size=1)\n",
    "        predictions = self.model.predict(test_generator)\n",
    "        \n",
    "        # Invert scaling for prediction data\n",
    "        predictions_inverted = self.scaler.inverse_transform(np.concatenate([predictions, np.zeros((predictions.shape[0], self.test.shape[1]-1))], axis=1))[:, 0]\n",
    "        \n",
    "        return predictions_inverted\n",
    "    \n",
    "    def visualize_and_evaluate(self, predictions, actual, start_idx=0):\n",
    "        actual_aligned = actual[start_idx:]\n",
    "        mae = mean_absolute_error(actual_aligned, predictions)\n",
    "        rmse = np.sqrt(mean_squared_error(actual_aligned, predictions))\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(actual_aligned, label='Actual', color='blue', marker='o')\n",
    "        plt.plot(predictions, label='Predicted', color='red', linestyle='--', marker='x')\n",
    "        plt.title('LSTM Time Series Prediction')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Value')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "        print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "    \n",
    "    def forecast_next_30_days(self):\n",
    "        # Use the last `look_back` observations to start the forecasting\n",
    "        last_observed_data = self.data_scaled[-self.look_back:]\n",
    "        forecast = []\n",
    "        for _ in range(30):  # Forecast for the next 30 days\n",
    "            x_last_observed = last_observed_data.reshape((1, self.look_back, self.data_scaled.shape[1]))\n",
    "            prediction = self.model.predict(x_last_observed)\n",
    "            forecast.append(prediction[0])\n",
    "            # Update last_observed_data with the new prediction\n",
    "            last_observed_data = np.vstack((last_observed_data[1:], prediction))\n",
    "        \n",
    "        # Invert scaling for forecasted data\n",
    "        forecast_inverted = self.scaler.inverse_transform(np.concatenate([np.array(forecast), np.zeros((len(forecast), self.data_scaled.shape[1]-1))], axis=1))[:, 0]\n",
    "        return forecast_inverted\n",
    "\n",
    "    def save_model(self, filepath):\n",
    "        # Save the model to disk\n",
    "        with open(filepath, 'wb') as file:\n",
    "            pickle.dump(self, file)\n",
    "\n",
    "    def visualize_forecast(self, forecasted_values):\n",
    "        # Combine train and test data for a complete view\n",
    "        complete_data = np.concatenate((self.train, self.test))\n",
    "        actual = self.scaler.inverse_transform(complete_data)[:, 0]  # Assuming the first column is the target variable\n",
    "        \n",
    "        # Prepare the index for plotting\n",
    "        time_steps = np.arange(len(actual) + len(forecasted_values))\n",
    "        \n",
    "        # Plot actual data\n",
    "        plt.figure(figsize=(15, 7))\n",
    "        plt.plot(time_steps[:len(actual)], actual, label='Historical Data', color='blue')\n",
    "        \n",
    "        # Plot forecasted data\n",
    "        plt.plot(time_steps[len(actual):], forecasted_values, label='Forecasted Next 30 Days', color='red', linestyle='--')\n",
    "        \n",
    "        plt.title('Historical Data and 30-Day Forecast')\n",
    "        plt.xlabel('Time Step')\n",
    "        plt.ylabel('Value')\n",
    "        plt.legend()\n",
    "        plt.show()    \n",
    "\n",
    "\n",
    "\n",
    "# Usage \n",
    "\n",
    "lstm_model = LSTMTimeSeriesModel(look_back=5, epochs=20, batch_size=20, lstm_units=50)\n",
    "lstm_model.preprocess_data(m1data)\n",
    "lstm_model.train_model()\n",
    "predictions = lstm_model.predict()\n",
    "\n",
    "actual = lstm_model.scaler.inverse_transform(lstm_model.test)[lstm_model.look_back:, 0] # Assuming the first column is the target variable\n",
    "\n",
    "# Visualize and evaluate the predictions\n",
    "lstm_model.visualize_and_evaluate(predictions, actual, start_idx=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***Observations:***\n",
    "\n",
    "- The graph plots two series: the actual values (in blue) and the predicted values (in red) of our target variable over time.\n",
    "\n",
    "- The actual and predicted lines follow a similar trend, suggesting that the LSTM model has captured the general pattern in the time series data.\n",
    "\n",
    "- There are points where the predicted values diverge from the actual values, indicating prediction errors. These points of divergence might be more frequent during periods of rapid change in the actual series.\n",
    "\n",
    "## ***Interpretation:***\n",
    "\n",
    "- Trend: The LSTM model seems to have learned the underlying trend in the data quite well. It captures the rises and falls in the time series, which is indicative of a well-fitted model.\n",
    "\n",
    "- MAE and RMSE: The MAE of 15.0793 and the RMSE of 16.5267 suggest that on average, the model's predictions are about 15 to 16.5 units off from the actual values. Whether this is acceptable depends on the specific context of the data and the scale of the target variable.\n",
    "\n",
    "- Volatility: The prediction seems to struggle more with the peaks and troughs, which are typically points of higher volatility in time series data. This could suggest that the model may need further tuning to better capture the volatility or that additional features or data could improve model performance.\n",
    "\n",
    "## ***Insights:***\n",
    "\n",
    "- Model Performance: Considering that the actual values range approximately from 100 to 220, an average error of 15 to 16.5 units could be seen as moderately good performance. However, if the target variable is highly sensitive to small changes, this level of error might be problematic.\n",
    "\n",
    "- Error Analysis: The points where the predicted values significantly deviate from the actual values might provide insights into where the model fails. Analyzing these points could reveal patterns or features that the model is currently not capturing.\n",
    "\n",
    "## ***Improvements: To improve the model, we'll consider:***\n",
    "\n",
    "- Feature Engineering: Adding more relevant features or transforming existing ones to provide the model with more information.\n",
    "\n",
    "- Hyperparameter Tuning: Adjusting the LSTM's hyperparameters, like the number of units, layers, or learning rate, to see if the model's performance can be improved.\n",
    "\n",
    "- Data Quality: Ensuring that the data fed into the model is clean, relevant, and representative of the problem space.\n",
    "\n",
    "- Complexity Handling: Incorporating techniques to handle complex patterns, such as attention mechanisms, or using a more complex model like a hybrid LSTM-CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'learn-env (Python 3.8.5)' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the TunedLSTMTimeSeriesModel class\n",
    "class TunedLSTMTimeSeriesModel:\n",
    "    def __init__(self, look_back=5, epochs=20, batch_size=20, lstm_units=50):\n",
    "        self.look_back = look_back\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.lstm_units = lstm_units\n",
    "        self.model = None\n",
    "        self.scaler = MinMaxScaler()\n",
    "\n",
    "    def preprocess_data(self, data):\n",
    "        # Normalize the data\n",
    "        self.data_scaled = self.scaler.fit_transform(data)\n",
    "        \n",
    "        # Split the dataset into training and testing sets\n",
    "        self.split_idx = int(len(self.data_scaled) * 0.8)\n",
    "        self.train, self.test = self.data_scaled[:self.split_idx], self.data_scaled[self.split_idx:]\n",
    "    \n",
    "    def create_model(self, lstm_units=50):\n",
    "        model = Sequential([\n",
    "            LSTM(lstm_units, activation='relu', input_shape=(self.look_back, self.train.shape[1])),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "        return model\n",
    "\n",
    "    def tune_hyperparameters(self, param_grid):\n",
    "        train_generator = TimeseriesGenerator(self.train, self.train[:, 0], length=self.look_back, batch_size=self.batch_size)\n",
    "        X, y = [], []\n",
    "        for i in range(len(train_generator)):\n",
    "            batch_x, batch_y = train_generator[i]\n",
    "            X.extend(batch_x)\n",
    "            y.extend(batch_y)\n",
    "        X, y = np.array(X), np.array(y)\n",
    "        \n",
    "        model = KerasRegressor(build_fn=lambda lstm_units=self.lstm_units: self.create_model(lstm_units), epochs=self.epochs, batch_size=self.batch_size, verbose=0)\n",
    "        grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error')\n",
    "        grid_result = grid.fit(X, y)\n",
    "        \n",
    "        print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "        self.model = self.create_model(grid_result.best_params_['lstm_units'])\n",
    "        self.lstm_units = grid_result.best_params_['lstm_units']\n",
    "        self.batch_size = grid_result.best_params_['batch_size']\n",
    "        self.epochs = grid_result.best_params_['epochs']\n",
    "\n",
    "    def train_model(self):\n",
    "        train_generator = TimeseriesGenerator(self.train, self.train[:, 0], length=self.look_back, batch_size=self.batch_size)\n",
    "        self.model.fit(train_generator, epochs=self.epochs, verbose=1)\n",
    "\n",
    "    def predict(self):\n",
    "        test_generator = TimeseriesGenerator(self.test, self.test[:, 0], length=self.look_back, batch_size=1)\n",
    "        predictions = self.model.predict(test_generator)\n",
    "        predictions_full = np.zeros((len(predictions), self.data_scaled.shape[1]))\n",
    "        predictions_full[:, 0] = predictions.ravel()\n",
    "        predictions_inverted = self.scaler.inverse_transform(predictions_full)[:, 0]\n",
    "        return predictions_inverted\n",
    "\n",
    "    def visualize_and_evaluate(self, predictions, actual):\n",
    "        mae = mean_absolute_error(actual, predictions)\n",
    "        rmse = np.sqrt(mean_squared_error(actual, predictions))\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(actual, label='Actual', color='blue', marker='o')\n",
    "        plt.plot(predictions, label='Predicted', color='red', linestyle='--', marker='x')\n",
    "        plt.title('LSTM Time Series Prediction')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Value')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "        print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "\n",
    "# Instantiate and preprocess data\n",
    "lstm_model = TunedLSTMTimeSeriesModel()\n",
    "lstm_model.preprocess_data(m1data)\n",
    "\n",
    "# Hyperparameters grid\n",
    "param_grid = {\n",
    "    'lstm_units': [20, 50, 100],\n",
    "    'batch_size': [16, 32, 64],\n",
    "    'epochs': [10, 20, 30]\n",
    "}\n",
    "\n",
    "# Tune hyperparameters\n",
    "lstm_model.tune_hyperparameters(param_grid)\n",
    "\n",
    "# Train the model\n",
    "lstm_model.train_model()\n",
    "\n",
    "# Predict and evaluate\n",
    "predictions_inverted = lstm_model.predict()\n",
    "actual_inverted = lstm_model.scaler.inverse_transform(lstm_model.test)[:, 0]\n",
    "lstm_model.visualize_and_evaluate(predictions_inverted, actual_inverted[len(predictions_inverted):])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'learn-env (Python 3.8.5)' due to a connection timeout. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DataNexus",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
